{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nithinrk11/FlowCast/blob/main/Necessary_script_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Combined code cell of the ML models"
      ],
      "metadata": {
        "id": "UQQn9FZOijlw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQjfE2IFibj5",
        "outputId": "e8047070-3180-48b1-a7e9-a1a21c17bd46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Cloning into 'FlowCast'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 16 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (16/16), 196.65 KiB | 762.00 KiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Best Hyperparameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}\n",
            "Epoch 1/20\n",
            "175/175 [==============================] - 7s 6ms/step - loss: 1.0141 - accuracy: 0.3715 - val_loss: 0.9146 - val_accuracy: 0.3807\n",
            "Epoch 2/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.7355 - accuracy: 0.6323 - val_loss: 0.3420 - val_accuracy: 0.9450\n",
            "Epoch 3/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.2472 - accuracy: 0.9032 - val_loss: 0.1698 - val_accuracy: 0.9379\n",
            "Epoch 4/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.2012 - accuracy: 0.9086 - val_loss: 0.1899 - val_accuracy: 0.9021\n",
            "Epoch 5/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1899 - accuracy: 0.9173 - val_loss: 0.1807 - val_accuracy: 0.9071\n",
            "Epoch 6/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1870 - accuracy: 0.9194 - val_loss: 0.1516 - val_accuracy: 0.9336\n",
            "Epoch 7/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1902 - accuracy: 0.9175 - val_loss: 0.1344 - val_accuracy: 0.9564\n",
            "Epoch 8/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1764 - accuracy: 0.9232 - val_loss: 0.1373 - val_accuracy: 0.9579\n",
            "Epoch 9/20\n",
            "175/175 [==============================] - 1s 7ms/step - loss: 0.1536 - accuracy: 0.9364 - val_loss: 0.3430 - val_accuracy: 0.8536\n",
            "Epoch 10/20\n",
            "175/175 [==============================] - 1s 6ms/step - loss: 0.1441 - accuracy: 0.9361 - val_loss: 0.1328 - val_accuracy: 0.9536\n",
            "Epoch 11/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1775 - accuracy: 0.9207 - val_loss: 0.1670 - val_accuracy: 0.9121\n",
            "Epoch 12/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1824 - accuracy: 0.9209 - val_loss: 0.1604 - val_accuracy: 0.9207\n",
            "Epoch 13/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1450 - accuracy: 0.9395 - val_loss: 0.1527 - val_accuracy: 0.9264\n",
            "Epoch 14/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1605 - accuracy: 0.9277 - val_loss: 0.1305 - val_accuracy: 0.9414\n",
            "Epoch 15/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1564 - accuracy: 0.9327 - val_loss: 0.3001 - val_accuracy: 0.8614\n",
            "Epoch 16/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1409 - accuracy: 0.9373 - val_loss: 0.1526 - val_accuracy: 0.9286\n",
            "Epoch 17/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1667 - accuracy: 0.9255 - val_loss: 0.1491 - val_accuracy: 0.9257\n",
            "Epoch 18/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1367 - accuracy: 0.9412 - val_loss: 0.1753 - val_accuracy: 0.9107\n",
            "Epoch 19/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1423 - accuracy: 0.9391 - val_loss: 0.1216 - val_accuracy: 0.9471\n",
            "Epoch 20/20\n",
            "175/175 [==============================] - 1s 5ms/step - loss: 0.1246 - accuracy: 0.9475 - val_loss: 0.1165 - val_accuracy: 0.9464\n",
            "55/55 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow scikit-learn\n",
        "!git clone https://github.com/nithinrk11/FlowCast.git\n",
        "import numpy as np\n",
        "\n",
        "#decision tree\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import pickle\n",
        "\n",
        "#random forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "#XGboost\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#XGboost Hyperparameter\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#DNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#Models\n",
        "dec = DecisionTreeClassifier()\n",
        "rf = RandomForestClassifier()\n",
        "xg = XGBClassifier()\n",
        "best_xgb = XGBClassifier()\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "\n",
        "# Load the noisy dataset\n",
        "data = pd.read_csv('/content/FlowCast/noisy_crowd_data2.csv', parse_dates=['Timestamp'])\n",
        "\n",
        "# Encode the 'Crowd_Type' column\n",
        "label_encoder = LabelEncoder()\n",
        "data['Crowd_Type_Label'] = label_encoder.fit_transform(data['Crowd_Type'])\n",
        "\n",
        "# Define features and target\n",
        "X = data[['Noisy_Crowd_Count']]\n",
        "y = data['Crowd_Type_Label']\n",
        "\n",
        "\n",
        "# Decision tree\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the decision tree model\n",
        "dec = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dec.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dec.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "#accuracy = accuracy_score(y_test, y_pred)\n",
        "#conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "#class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "#print(f\"Decision tree Accuracy: {accuracy:.2f}\")\n",
        "#print(\"\\nDecision tree Confusion Matrix:\")\n",
        "#print(conf_matrix)\n",
        "#print(\"\\nDecision tree Classification Report:\")\n",
        "#print(class_report)\n",
        "#-----------------------------------------------------------------------------#\n",
        "\n",
        "# random forest\n",
        "# Binarize the output for multiclass classification\n",
        "y_bin = label_binarize(y, classes=np.unique(y))\n",
        "\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "#accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "#print(f\"Random Forest Accuracy: {accuracy_rf:.2f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "#conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "#print(\"\\nRandom Forest Confusion Matrix:\")\n",
        "#print(conf_matrix_rf)\n",
        "\n",
        "# Classification Report\n",
        "#class_report_rf = classification_report(y_test, y_pred_rf)\n",
        "#print(\"\\nRandom Forest Classification Report:\")\n",
        "#print(class_report_rf)\n",
        "#----------------------------------------------------------------------------#\n",
        "# XGBoost\n",
        "# Set the number of classes explicitly\n",
        "num_classes = len(np.unique(y))\n",
        "\n",
        "# Create an XGBoost classifier with num_class parameter\n",
        "xg = XGBClassifier(num_class=num_classes)\n",
        "\n",
        "# Train the model\n",
        "xg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xg.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "#accuracy = accuracy_score(y_test, y_pred)\n",
        "#print(f\"XGBoost Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "#conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "#print(\"\\nXGBoost Confusion Matrix:\")\n",
        "#print(conf_matrix)\n",
        "\n",
        "# Classification Report\n",
        "#class_report = classification_report(y_test, y_pred)\n",
        "#print(\"\\nXGBoost Classification Report:\")\n",
        "#print(class_report)\n",
        "#---------------------------------------------------------------------------#\n",
        "\n",
        "# XGBoost Hyperparameter\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0],\n",
        "}\n",
        "\n",
        "# Create an XGBoost classifier\n",
        "xgb = XGBClassifier(random_state=42)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring='accuracy', cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Use the best model for prediction\n",
        "best_xgb = grid_search.best_estimator_\n",
        "y_pred = best_xgb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "#accuracy = accuracy_score(y_test, y_pred)\n",
        "#print(f\"Tuned XGBoost Accuracy with Best Model: {accuracy:.2f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "#conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "#print(\"\\nTuned XGBoost Confusion Matrix:\")\n",
        "#print(conf_matrix)\n",
        "\n",
        "# Classification Report\n",
        "#class_report = classification_report(y_test, y_pred)\n",
        "#print(\"\\nTuned XGBoost Classification Report:\")\n",
        "#print(class_report)\n",
        "#----------------------------------------------------------------------------#\n",
        "\n",
        "# DNN\n",
        "# Set random seed for NumPy\n",
        "np.random.seed(42)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_one_hot = to_categorical(y_train)\n",
        "y_test_one_hot = to_categorical(y_test)\n",
        "\n",
        "# Build the DNN model\n",
        "\n",
        "model.add(Dense(128, input_dim=1, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(4, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))  # Adjusted output layer for three classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train_one_hot, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "y_test_labels = np.argmax(y_test_one_hot, axis=1)\n",
        "\n",
        "# Calculate metrics\n",
        "#accuracy = accuracy_score(y_test_labels, y_pred_labels)\n",
        "#conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
        "#class_report = classification_report(y_test_labels, y_pred_labels)\n",
        "\n",
        "#print(f\"DNN Accuracy: {accuracy:.2f}\")\n",
        "#print(\"\\nDNN Confusion Matrix:\")\n",
        "#print(conf_matrix)\n",
        "#print(\"\\nDNN Classification Report:\")\n",
        "#print(class_report)\n",
        "\n",
        "#Dumping the models into correspondind pickle file in write mode\n",
        "pickle.dump(dec,open('dec.pkl','wb'))\n",
        "pickle.dump(rf,open('rf.pkl','wb'))\n",
        "pickle.dump(xg,open('xg.pkl','wb'))\n",
        "pickle.dump(best_xgb,open('best_xgb.pkl','wb'))\n",
        "pickle.dump(model,open('model.pkl','wb'))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Streamlit code\n",
        "* The below is the most simplified edit and first stage of web-app design edit where the data input is done by a single crowd count number from the slider option by the user.\n",
        "* `New updated code edits are being done and will be updated further below.`"
      ],
      "metadata": {
        "id": "G3neZEKQ0LWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "dec = pickle.load(open('dec.pkl', 'rb'))\n",
        "rf = pickle.load(open('rf.pkl', 'rb'))\n",
        "xg = pickle.load(open('xg.pkl', 'rb'))\n",
        "best_xgb = pickle.load(open('best_xgb.pkl', 'rb'))\n",
        "model = pickle.load(open('model.pkl', 'rb'))\n",
        "\n",
        "def classify(num, model):\n",
        "    # Reshape the input to a 2D array\n",
        "    inputs = np.array([num]).reshape(1, -1)\n",
        "\n",
        "    if num <= 30:\n",
        "        return \"Low Crowd\"\n",
        "    elif 30 < num <= 70:\n",
        "        return \"Moderate Crowd\"\n",
        "    else:\n",
        "        return \"High Crowd\"\n",
        "\n",
        "def main():\n",
        "    st.title(\"Streamlit Tutorial\")\n",
        "    html_temp = \"\"\"\n",
        "    <div style=\"background-color:teal ;padding:10px\">\n",
        "    <h2 style=\"color:white;text-align:center;\">Crowd Classification</h2>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    st.markdown(html_temp, unsafe_allow_html=True)\n",
        "    activities = ['Decision tree', 'Random forest', 'XGBoost', 'Tuned XGBoost', 'DNN']\n",
        "    option = st.sidebar.selectbox('Which model would you like to use?', activities)\n",
        "    st.subheader(option)\n",
        "    sl = st.slider('Crowd count', 10, 150)\n",
        "    if st.button('Classify'):\n",
        "        st.success(classify(sl, dec) if option == 'Decision tree' else\n",
        "                  classify(sl, rf) if option == 'Random forest' else\n",
        "                  classify(sl, xg) if option == 'XGBoost' else\n",
        "                  classify(sl, best_xgb) if option == 'Tuned XGBoost' else\n",
        "                  classify(sl, model))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "eniQQRuB0FSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UPDATES"
      ],
      "metadata": {
        "id": "BESSPU-VybbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Updated Streamlit code with improved informative detailing and input data is uploaded as CSV file for classification\n",
        "* Further improvements are expected to be done in future untill final decision of edit"
      ],
      "metadata": {
        "id": "npC1efxGw6ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the models\n",
        "dec = pickle.load(open('dec.pkl', 'rb'))\n",
        "rf = pickle.load(open('rf.pkl', 'rb'))\n",
        "xg = pickle.load(open('xg.pkl', 'rb'))\n",
        "best_xgb = pickle.load(open('best_xgb.pkl', 'rb'))\n",
        "model = pickle.load(open('model.pkl', 'rb'))\n",
        "\n",
        "def classify(num, model):\n",
        "    if num <= 30:\n",
        "        return \"Low Crowd\"\n",
        "    elif 30 < num <= 70:\n",
        "        return \"Moderate Crowd\"\n",
        "    else:\n",
        "        return \"High Crowd\"\n",
        "\n",
        "def classify_range(crowd_counts, model):\n",
        "    results = [classify(num, model) for num in crowd_counts]\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    st.title(\"Project FlowCast\")\n",
        "    html_temp = \"\"\"\n",
        "    <div style=\"background-color:teal ;padding:10px\">\n",
        "    <h2 style=\"color:white;text-align:center;\">Crowd Classification</h2>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    st.markdown(html_temp, unsafe_allow_html=True)\n",
        "    activities = ['Decision tree', 'Random forest', 'XGBoost', 'Tuned XGBoost', 'DNN']\n",
        "    option = st.sidebar.selectbox('Which model would you like to use?', activities)\n",
        "    st.subheader(option)\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Upload CSV file\", type=[\"csv\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        data = pd.read_csv(uploaded_file)\n",
        "\n",
        "        # Assuming the column containing crowd counts is named 'Crowd_Count'\n",
        "        if 'Crowd_Count' in data.columns and 'Timestamp' in data.columns:\n",
        "            data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
        "\n",
        "            # Resample the data for daily, weekly, and monthly averages\n",
        "            daily_avg = data.resample('D', on='Timestamp').mean()\n",
        "            weekly_avg = data.resample('W-Mon', on='Timestamp').mean()\n",
        "            monthly_avg = data.resample('M', on='Timestamp').mean()\n",
        "\n",
        "            crowd_counts = data['Crowd_Count'].tolist()\n",
        "            timestamps = data['Timestamp']\n",
        "\n",
        "            results = classify_range(crowd_counts, dec) if option == 'Decision tree' else \\\n",
        "                      classify_range(crowd_counts, rf) if option == 'Random forest' else \\\n",
        "                      classify_range(crowd_counts, xg) if option == 'XGBoost' else \\\n",
        "                      classify_range(crowd_counts, best_xgb) if option == 'Tuned XGBoost' else \\\n",
        "                      classify_range(crowd_counts, model)\n",
        "\n",
        "            # Calculate overall average crowd count\n",
        "            overall_avg = np.mean(crowd_counts)\n",
        "\n",
        "            # Display the time versus crowd count graph with different colors for each crowd type\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            sns.lineplot(x=timestamps, y=crowd_counts, hue=results, palette=\"viridis\", ax=ax, label='Crowd Count')\n",
        "\n",
        "            # Plot average trend lines\n",
        "            sns.lineplot(x=daily_avg.index, y=daily_avg['Crowd_Count'], color='blue', label='Daily Avg', ax=ax)\n",
        "            sns.lineplot(x=weekly_avg.index, y=weekly_avg['Crowd_Count'], color='green', label='Weekly Avg', ax=ax)\n",
        "            sns.lineplot(x=monthly_avg.index, y=monthly_avg['Crowd_Count'], color='orange', label='Monthly Avg', ax=ax)\n",
        "\n",
        "            # Plot overall average line\n",
        "            plt.axhline(overall_avg, color='red', linestyle='--', label='Overall Avg')\n",
        "\n",
        "            plt.title('Time vs Crowd Count with Average Estimation')\n",
        "            plt.xlabel('Timestamp')\n",
        "            plt.ylabel('Crowd Count')\n",
        "            plt.legend()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            # Display the crowd count classifications in a table\n",
        "            results_df = pd.DataFrame({'Timestamp': timestamps, 'Crowd_Count': crowd_counts, 'Classification': results})\n",
        "            st.write(results_df)\n",
        "        else:\n",
        "            st.error(\"Please make sure your CSV file has columns named 'Timestamp' and 'Crowd_Count'.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "gW-pqqOyxQ4N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}